# estia

#### user attention estimation bundle for the _2RealFramework

*estia* (greek for "focus") receives input from a Kinectv2 stream via OSC and estimates any number of visible persons' attention levels, encoded in two variables, the *short time frame* (STF) and the *long time frame* (LTF).

## System

For the [_2RealFamework](https://github.com/cadet/_2RealFramework). Only tested on Windows (7+), as the framework itself is only tested on Windows.

## Concept

Based on current research in the field of unobtrusive automatic estimation of human attention from the analysis of optical tracking (most centrally *Gollan et al, 2011*), this bundle uses supervised multi-class machine learning techniques in the shape of Support Vector Machines (SVMs) to predict short-term and long-term attention levels.

It is conceived to be used with a single depth camera and an application streaming tracking data using a specific protocol (see below).

It features two main modes of operation:

- **COLLECT** (mode 0): In this mode, a human operator should supply STF and LTF class labels for all subjects as they are tracked by the system. The results will be saved in data files.
- **PREDICT** (mode 1): This mode is used for estimating attention based on the data gathered in *COLLECT* mode. At startup, the SVMs are trained with data from files saved in *COLLECT* mode. Then, every frame, STF and LTF predictions are made for each subject, and provided via a block outlet.

## Quick Start

See `src/TestApp_Estia/TestApp_Estia.cpp`.

## Types

The most important type exported by this bundle is `attentive`. An `attentive` is a human subject, featuring:
- *id*, a unique identifier type exported by the **IdentificationBundle** (*TODO* link)
- *pos*, a 2D position vector (ground plane, relative to the tracking camera)
- *attentionStf*, uint8, an STF attention class [0-5]
- *attentionLtf*, uint8, an LTF attention class [0-4]

The attention classes only contain meaningful data when in **PREDICT** mode. See below for detailed meaning of the class numbers.

There are two types for configuring the blocks, `trackingConfig` and `modellingConfig`; see the TestApp for details.

## Blocks

The bundle provides two blocks: `attentionTracking` and `attentionModelling`.

### attentionTracking

Listens to OSC data (via [oscPack][]) conforming to the following protocol:

    address:
            /body
    data:
            body_id                int 
            base_roll              float 
            midspline_roll         float
            midshoulder_roll       float
            neck_roll              float
            base_x                 float
            base_y                 float
            base_z                 float
            face_rot_yaw           float
            face_rot_pitch         float
            face_rot_roll          float
            face_engaged           int    // 0 = Unknown, 1 = No, 2 = Maybe, 3 = Yes
                
Such data is generated by the K4W2Streamer (*TODO* link), an application streaming condensed Kinectv2 tracking information over the network.

The block transforms this data to an internally useful format. Its only outlet should be connected to an `attentionModelling` block.

### attentionModelling

This block performs the actual feature extraction by calculating means and standard deviations of variousparameters extracted from the streamed input values. It also manages all of the SVM training and prediction functionality (provided by [libSVM][]).

In *COLLECT* mode, its second inlet must be fed (in realtime) with data of type `humanLabel`, consisting of an id type (see above) and an STF label for the subject with that id. 

*There is currently no way to label LTF patterns via a block interface. For now, LTF patterns should be manually labelled after the fact, by manipulating the produced data file - there will only be one LTF pattern per subject.*

In both modes, its only outlet provides `attentive`s, but only in *PREDICT* mode will these have valid STF and LTF predictions attached.

## Attention Classes

### STF classed:

1. OUTOFVIEW:: no visual contact
2. PERIPHERAL :: peripheral visual contact, movement away from screen
3. APPROACHING :: change in movement, turning towards screen, approaching
4. FOCUS :: close distance, focus on screen, interacting
5. LEAVING :: change in movement, turning away, leaving

### LTF tags:

1. IGNORER :: ignorer (object not in FOV or no perceivable eye contact)
2. GLIMPSER :: glimpser (torso or head turning, no movement change)
3. WATCHER :: watcher (slowing down, approaching, stopping)
4. INTERACTER :: interacter (interaction with screen)

In both cases, an integer value of 0 denotes no class, and should not be used as a label.

All labelled feature data is stored in the `data/` subfolder, using the naming scheme `YYYY-mm-dd_HH-MM-SS_(ltf|stf).txt`.

## Internals

### Features used for classification

    "vel_mag"       velocity magnitude
    "rot",          subject's torso rotation
    "accel_mag"     acceleration magnitude
    "phi"           movement direction relative to the camera
    "distance"      distance to the camera
    "directness"    (position - initial_position) / total_distance_travelled
    "facerot"       face rotation relative to the camera
    "engaged"       judged from the face by kinect SDK, using face and eye characteristics

For each feature, mean and standard deviation is used, i.e. 16 features in total.

For STF, the last 3 frames are used for calculation, and there is one pattern per frame. 
For LTF, all frames are used for calculation, and there is one pattern per subject.

Note that in the SVM data files, the features are sorted alphabetically by name (due to `std::map behaviour`), i.e. index 1 is `accel_mag` mean, index 2 is `accel_mag` stddev, index 3 is `directness` mean etc.

### Libraries used

- [libSVM][]
- [oscPack][]
- [Eigen][]

All three are redistributed in source form.

[libSVM]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/

[oscPack]: http://www.rossbencina.com/code/oscpack

[Eigen]: http://eigen.tuxfamily.org/

### References

- B. Gollan, B. Wally, and A. Ferscha. Automatic Attention Estimation in an Interactive System based on Behaviour Analysis. In *Proceedings of the 15th Portuguese Conference on Artificial Intelligence (EPIA2011)*, Jan. 2011.
